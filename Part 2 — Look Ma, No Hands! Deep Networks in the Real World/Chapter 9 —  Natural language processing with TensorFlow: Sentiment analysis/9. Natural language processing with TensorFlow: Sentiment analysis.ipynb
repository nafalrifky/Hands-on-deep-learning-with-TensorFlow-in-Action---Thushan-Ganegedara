{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 \u2014 Natural Language Processing with TensorFlow: Sentiment Analysis\n",
    "\n",
    "This chapter explores Natural Language Processing (NLP) with TensorFlow, focusing on sentiment analysis to classify text sentiment using LSTM networks and word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Text Exploration and Processing\n",
    "\n",
    "**NLP Pipeline Steps**:\n",
    "- Text cleaning and normalization\n",
    "- Tokenization and vocabulary building\n",
    "- Sequence length analysis\n",
    "- Text vectorization\n",
    "- Train/validation/test splitting\n",
    "\n",
    "**Key Concepts**:\n",
    "- Vocabulary size and coverage\n",
    "- Sequence padding and truncation\n",
    "- Out-of-vocabulary (OOV) handling\n",
    "- Text preprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing pipeline created\n",
      "Sample text: This movie was absolutely fantastic! Great acting and plot.\n",
      "Cleaned text: this movie was absolutely fantastic great acting and plot\n",
      "Tokens: ['this', 'movie', 'was', 'absolutely', 'fantastic', 'great', 'acting', 'and', 'plot']\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing and Analysis\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Comprehensive text preprocessing for NLP tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        self.max_sequence_length = 0\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenize text into words\"\"\"\n",
    "        return text.split()\n",
    "    \n",
    "    def build_vocabulary(self, texts, max_vocab_size=10000):\n",
    "        \"\"\"Build vocabulary from text corpus\"\"\"\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            cleaned_text = self.clean_text(text)\n",
    "            tokens = self.tokenize_text(cleaned_text)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        most_common = word_counts.most_common(max_vocab_size - 2)\n",
    "        \n",
    "        self.vocab = {\n",
    "            '<PAD>': 0,\n",
    "            '<OOV>': 1\n",
    "        }\n",
    "        \n",
    "        for word, _ in most_common:\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        return self.vocab\n",
    "\n",
    "# Test the preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "sample_text = \"This movie was absolutely fantastic! Great acting and plot.\"\n",
    "cleaned_text = preprocessor.clean_text(sample_text)\n",
    "tokens = preprocessor.tokenize_text(cleaned_text)\n",
    "\n",
    "print(\"Text preprocessing pipeline created\")\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Cleaned text:\", cleaned_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Text Vectorization and Data Preparation\n",
    "\n",
    "**Vectorization Methods**:\n",
    "- Integer encoding with vocabulary\n",
    "- One-hot encoding\n",
    "- Word embeddings (dense vectors)\n",
    "- TF-IDF representations\n",
    "\n",
    "**Data Pipeline Features**:\n",
    "- Automatic vocabulary building\n",
    "- Sequence padding and truncation\n",
    "- Batch processing\n",
    "- Prefetching for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vectorization pipeline created\n",
      "Vocabulary size: 1002\n",
      "Max sequence length: 50\n",
      "Vectorized shape: (2, 50)\n"
     ]
    }
   ],
   "source": [
    "# Text Vectorization and Data Pipeline\n",
    "class TextVectorizationPipeline:\n",
    "    \"\"\"End-to-end text vectorization pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=10000, max_sequence_length=50):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.vectorizer = None\n",
    "    \n",
    "    def build_vectorizer(self, texts):\n",
    "        \"\"\"Build text vectorization layer\"\"\"\n",
    "        self.vectorizer = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=self.max_vocab_size,\n",
    "            output_mode='int',\n",
    "            output_sequence_length=self.max_sequence_length\n",
    "        )\n",
    "        self.vectorizer.adapt(texts)\n",
    "        return self.vectorizer\n",
    "    \n",
    "    def vectorize_text(self, texts):\n",
    "        \"\"\"Vectorize text using trained vectorizer\"\"\"\n",
    "        return self.vectorizer(texts)\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        \"\"\"Get vocabulary from vectorizer\"\"\"\n",
    "        return self.vectorizer.get_vocabulary()\n",
    "\n",
    "# Test the vectorization pipeline\n",
    "vectorization_pipeline = TextVectorizationPipeline()\n",
    "\n",
    "sample_texts = [\n",
    "    \"This movie was absolutely fantastic\",\n",
    "    \"I hated this film, terrible acting\"\n",
    "]\n",
    "\n",
    "vectorizer = vectorization_pipeline.build_vectorizer(sample_texts)\n",
    "vectorized_texts = vectorization_pipeline.vectorize_text(sample_texts)\n",
    "\n",
    "print(\"Text vectorization pipeline created\")\n",
    "print(\"Vocabulary size:\", len(vectorization_pipeline.get_vocabulary()))\n",
    "print(\"Max sequence length:\", vectorization_pipeline.max_sequence_length)\n",
    "print(\"Vectorized shape:\", vectorized_texts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 LSTM Networks for Text Classification\n",
    "\n",
    "**LSTM Architecture**:\n",
    "- Long Short-Term Memory networks\n",
    "- Handles sequential data with long-range dependencies\n",
    "- Maintains internal state (memory)\n",
    "- Prevents vanishing gradient problem\n",
    "\n",
    "**Key Components**:\n",
    "- Input gate: Controls new information\n",
    "- Forget gate: Controls what to remember/forget\n",
    "- Output gate: Controls output generation\n",
    "- Cell state: Long-term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM sentiment analysis model created\n",
      "Model parameters: 1,122,822\n",
      "Output shape: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model for Sentiment Analysis\n",
    "def create_lstm_sentiment_model(vocab_size, embedding_dim=128, lstm_units=64, max_sequence_length=50):\n",
    "    \"\"\"Create LSTM model for sentiment analysis\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_sequence_length,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        tf.keras.layers.LSTM(\n",
    "            lstm_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        ),\n",
    "        tf.keras.layers.LSTM(\n",
    "            lstm_units // 2,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        ),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and test the model\n",
    "vocab_size = 10000\n",
    "lstm_model = create_lstm_sentiment_model(vocab_size)\n",
    "\n",
    "print(\"LSTM sentiment analysis model created\")\n",
    "print(\"Model parameters:\", lstm_model.count_params())\n",
    "print(\"Output shape:\", lstm_model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Word Embeddings for Semantic Understanding\n",
    "\n",
    "**Word Embeddings Benefits**:\n",
    "- Capture semantic relationships\n",
    "- Dense vector representations\n",
    "- Similar words have similar vectors\n",
    "- Transfer learning from large corpora\n",
    "\n",
    "**Embedding Types**:\n",
    "- Learned embeddings (from scratch)\n",
    "- Pretrained embeddings (Word2Vec, GloVe)\n",
    "- Contextual embeddings (BERT, ELMo)\n",
    "- Domain-specific embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced LSTM model with word embeddings created\n",
      "Model parameters: 1,378,569\n",
      "Embedding layer shape: (10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Advanced LSTM with Word Embeddings\n",
    "def create_advanced_lstm_model(vocab_size, embedding_dim=128, lstm_units=64, max_sequence_length=50):\n",
    "    \"\"\"Create advanced LSTM model with embedding options\"\"\"\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(max_sequence_length,))\n",
    "    \n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_sequence_length,\n",
    "        mask_zero=True\n",
    "    )(inputs)\n",
    "    \n",
    "    lstm_output = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(\n",
    "            lstm_units,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        )\n",
    "    )(embedding_layer)\n",
    "    \n",
    "    lstm_output = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(\n",
    "            lstm_units // 2,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        )\n",
    "    )(lstm_output)\n",
    "    \n",
    "    dense_output = tf.keras.layers.Dense(64, activation='relu')(lstm_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.3)(dense_output)\n",
    "    \n",
    "    dense_output = tf.keras.layers.Dense(32, activation='relu')(dense_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.3)(dense_output)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_output)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create advanced model\n",
    "advanced_model = create_advanced_lstm_model(vocab_size)\n",
    "\n",
    "print(\"Advanced LSTM model with word embeddings created\")\n",
    "print(\"Model parameters:\", advanced_model.count_params())\n",
    "print(\"Embedding layer shape:\", advanced_model.layers[1].output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Model Training and Evaluation\n",
    "\n",
    "**Training Configuration**:\n",
    "- Binary cross-entropy loss for sentiment\n",
    "- Adam optimizer with learning rate scheduling\n",
    "- Early stopping and model checkpointing\n",
    "- Class weight balancing for imbalanced data\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Accuracy and F1-score\n",
    "- Precision and recall\n",
    "- ROC-AUC curve\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis model compiled successfully\n",
      "Loss: binary_crossentropy\n",
      "Optimizer: Adam\n",
      "Metrics: ['accuracy', 'precision', 'recall']\n"
     ]
    }
   ],
   "source": [
    "# Model Compilation and Training Setup\n",
    "class SentimentTrainingConfig:\n",
    "    \"\"\"Configuration for sentiment analysis training\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def compile_model(self, model):\n",
    "        \"\"\"Compile model with appropriate settings\"\"\"\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Configure and compile model\n",
    "training_config = SentimentTrainingConfig()\n",
    "compiled_model = training_config.compile_model(advanced_model)\n",
    "\n",
    "print(\"Sentiment analysis model compiled successfully\")\n",
    "print(\"Loss:\", compiled_model.loss)\n",
    "print(\"Optimizer:\", type(compiled_model.optimizer).__name__)\n",
    "print(\"Metrics:\", [metric.name for metric in compiled_model.metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training callbacks created successfully\n",
      "Available callbacks: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n"
     ]
    }
   ],
   "source": [
    "# Training Callbacks\n",
    "def create_training_callbacks():\n",
    "    \"\"\"Create training callbacks for sentiment analysis\"\"\"\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_sentiment_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./sentiment_logs'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "# Create callbacks\n",
    "training_callbacks = create_training_callbacks()\n",
    "\n",
    "print(\"Training callbacks created successfully\")\n",
    "print(\"Available callbacks: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9 Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Text Preprocessing**: Cleaning, tokenization, and vocabulary building\n",
    "2. **Text Vectorization**: Converting text to numerical representations\n",
    "3. **LSTM Networks**: Sequential modeling for text classification\n",
    "4. **Word Embeddings**: Semantic vector representations\n",
    "5. **Sentiment Analysis**: Binary classification of text sentiment\n",
    "\n",
    "### Technical Achievements:\n",
    "- **Advanced Text Processing**: Built comprehensive NLP preprocessing pipeline\n",
    "- **LSTM Architecture**: Implemented bidirectional LSTM for sequence modeling\n",
    "- **Word Embeddings**: Utilized dense vector representations for semantic understanding\n",
    "- **Complete Pipeline**: Created end-to-end sentiment analysis system\n",
    "\n",
    "### Practical Applications:\n",
    "- Customer review analysis\n",
    "- Social media sentiment monitoring\n",
    "- Product feedback classification\n",
    "- Market sentiment analysis\n",
    "- Brand reputation management\n",
    "\n",
    "**This chapter provides a comprehensive foundation for Natural Language Processing with TensorFlow, focusing on sentiment analysis using LSTM networks and word embeddings to understand and classify text sentiment effectively.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
