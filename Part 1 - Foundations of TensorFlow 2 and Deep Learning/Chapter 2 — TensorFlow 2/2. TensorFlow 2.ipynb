{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2 — TensorFlow 2\n",
        "This chapter provides a compact explanation of how TensorFlow 2 works under the hood, how it executes code, and how core neural-network operations are defined. All figures from the textbook are included in the correct order."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 First steps with TensorFlow 2\n",
        "TensorFlow 2 simplifies machine-learning development by enabling eager execution by default, meaning operations execute immediately and behave like standard Python code. This makes the development workflow more intuitive, especially for beginners.\n",
        "\n",
        "To introduce the core ideas, the chapter begins with a simple Multilayer Perceptron (MLP). An MLP consists of an input layer, one or more hidden layers, and an output layer. TensorFlow represents all computations—matrix multiplication, activation functions, and softmax—as tensor operations executed on CPU, GPU, or TPU."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.1 — MLP architecture\n",
        "<p align='left'><img src='./figure/figure2.1.png' width='60%'></p>\n",
        "This diagram illustrates a basic feed-forward neural network: inputs are multiplied by weights, transformed by nonlinear activations, and passed to the next layer. TensorFlow expresses all these steps as tensor operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.2 Activation functions\n",
        "The MLP uses nonlinear activation functions such as the sigmoid or ReLU. These nonlinearities allow the network to learn complex patterns rather than simple linear mappings."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.2 — Sigmoid activation\n",
        "<p align='left'><img src='./figure/figure2.2.png' width='60%'></p>\n",
        "The sigmoid function maps any real number to a range between 0 and 1, making it useful for binary classification or as a smooth gating function."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.3 — Softmax normalization\n",
        "<p align='left'><img src='./figure/figure2.3.png' width='60%'></p>\n",
        "Softmax converts raw output scores into probabilities that sum to 1. It is commonly used in multi-class classification tasks."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 TensorFlow computation model\n",
        "TensorFlow expresses numerical computation using three core building blocks: tensors, variables, and operations. These components form the foundation of all neural network implementations in TensorFlow.\n",
        "\n",
        "TensorFlow 2 executes eagerly by default, but can switch to graph-based execution via `@tf.function` to optimize performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2.1 Tensors, Variables, and Operations\n",
        "TensorFlow programs are composed of three elements:\n",
        "- **tf.Tensor:** immutable data arrays used for inputs and intermediate activations.\n",
        "- **tf.Variable:** mutable tensors whose values can change during training.\n",
        "- **tf.Operation:** functions that transform tensors (e.g., `tf.matmul`, `tf.add`).\n",
        "\n",
        "These building blocks are combined into larger computational graphs during execution."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.4 — Basic computation graph\n",
        "<p align='left'><img src='./figure/figure2.4.png' width='60%'></p>\n",
        "This figure shows how tensors and operations connect to form a directed graph representing the computation pipeline."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Graph tracing with `@tf.function`\n",
        "Although TensorFlow runs eagerly, developers can add `@tf.function` to transform Python functions into optimized computational graphs. This improves execution speed, especially for repeated calculations such as training loops.\n",
        "\n",
        "`@tf.function` traces a function on its first execution and then reuses the generated graph on subsequent calls."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.5 — Tracing and execution flow\n",
        "<p align='left'><img src='./figure/figure2.5.png' width='60%'></p>\n",
        "The diagram illustrates how TensorFlow traces Python code into a graph on the first call, then executes the optimized graph on later calls."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3.1 AutoGraph conversion\n",
        "TensorFlow's AutoGraph system automatically converts Python control flow (`if`, `for`, `while`) into TensorFlow operations such as `tf.cond` and `tf.while_loop`.\n",
        "\n",
        "This allows high-level Python logic to be executed efficiently on GPU/TPU as part of a unified graph."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.6 — AutoGraph transforming Python control flow\n",
        "<p align='left'><img src='./figure/figure2.6.png' width='60%'></p>\n",
        "The figure demonstrates how TensorFlow rewrites Python loops and conditionals into graph-compatible operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Supporting tools in the TensorFlow ecosystem\n",
        "TensorFlow provides powerful tools that improve developer productivity and model monitoring. Two commonly used components are **TensorBoard** and **TensorFlow Hub**.\n",
        "\n",
        "These tools help visualize training metrics, inspect model graphs, download pretrained models, and reuse standardized components."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4.1 TensorBoard\n",
        "TensorBoard is TensorFlow’s built-in visualization toolkit. It can display training curves, histograms, computation graphs, and profiling information.\n",
        "\n",
        "Visualization with TensorBoard helps diagnose issues like overfitting, vanishing gradients, or unstable learning rates."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.7 — TensorBoard visualization example\n",
        "<p align='left'><img src='./figure/figure2.7.png' width='60%'></p>\n",
        "This figure shows how TensorBoard displays model graphs and metric curves during training."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4.2 TensorFlow Hub\n",
        "TensorFlow Hub provides reusable pretrained components such as image feature extractors, text embeddings, and fine-tunable models.\n",
        "\n",
        "Using Hub modules reduces development time and ensures model reproducibility across different projects."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.8 — TensorFlow Hub module flow\n",
        "<p align='left'><img src='./figure/figure2.8.png' width='60%'></p>\n",
        "The diagram illustrates how TensorFlow Hub wraps pretrained layers and exposes them as reusable modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Neural network computations in TensorFlow\n",
        "TensorFlow provides efficient implementations for neural-network operations such as matrix multiplication, convolution, and pooling. These operations form the computational core of most deep learning architectures."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5.1 Matrix multiplication (`tf.matmul`)\n",
        "Matrix multiplication lies at the heart of dense (fully connected) layers. Inputs, weights, and biases are combined linearly before applying nonlinear activation functions.\n",
        "\n",
        "Dense layers compute:  \n",
        "$$y = xW + b$$\n",
        "TensorFlow automatically dispatches this computation to optimized kernels (CPU, GPU, or TPU)."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.9 — Dense layer computation\n",
        "<p align='left'><img src='./figure/figure2.9.png' width='60%'></p>\n",
        "This figure illustrates how inputs are multiplied with weights and shifted by biases before passing through an activation function."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5.2 Convolution (`tf.nn.conv2d`)\n",
        "Convolution layers extract spatial features by sliding a kernel over an image or feature map. Important parameters include:\n",
        "- Kernel size\n",
        "- Stride\n",
       "- Padding (`SAME` or `VALID`)\n",
        "\n",
        "Convolution reduces the need for large fully connected layers by sharing weights across spatial positions."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.10 — Convolution operation\n",
        "<p align='left'><img src='./figure/figure2.10.png' width='60%'></p>\n",
        "This diagram shows how a convolution kernel slides over an input matrix to generate feature maps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5.3 Pooling operations (`tf.nn.max_pool2d` / `tf.nn.avg_pool2d`)\n",
        "Pooling reduces spatial dimensions while preserving key features. This helps reduce computation, prevent overfitting, and make representations more robust to shifts.\n",
        "\n",
        "Two common pooling types are:\n",
        "- **Max pooling:** selects the maximum value in each window.\n",
        "- **Average pooling:** computes the mean within each window.\n",
        "\n",
        "Pooling typically follows convolution layers in CNN architectures."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.11 — Pooling example\n",
        "<p align='left'><img src='./figure/figure2.11.png' width='60%'></p>\n",
        "This image shows how max-pooling reduces the size of the feature map while preserving high-activation regions."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional convolution/pooling outputs\n",
        "The remaining figures illustrate how convolution and pooling combine to progressively reduce spatial resolution while increasing feature abstraction."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.12 — Convolution + pooling pipeline\n",
        "<p align='left'><img src='./figure/figure2.12.png' width='60%'></p>\n",
        "The figure highlights how sequential conv–pool layers extract increasingly complex patterns from images."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2.13 — Final feature map visualization\n",
        "<p align='left'><img src='./figure/figure2.13.png' width='60%'></p>\n",
        "This visualization demonstrates how deep convolutional feature maps represent different visual structures across layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 2 Summary\n",
        "TensorFlow 2 introduces an execution model that balances ease of use and high performance. Eager execution provides an intuitive development experience, while `@tf.function` allows TensorFlow to compile Python code into optimized graphs.\n",
        "\n",
        "Neural networks in TensorFlow rely heavily on operations such as matrix multiplication, convolution, and pooling—all efficiently implemented on modern hardware accelerators. Tools like TensorBoard and TensorFlow Hub further enhance the development pipeline by offering visualization and reusable pretrained models.\n",
        "\n",
        "By understanding how tensors, variables, and operations form computational graphs, developers gain insight into how TensorFlow executes models efficiently across CPUs, GPUs, and TPUs."
      ]
    }
  ]
}
